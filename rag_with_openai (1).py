# -*- coding: utf-8 -*-
"""RAG with OpenAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B8BSdXCm4Q106vpghCbF7JCr88k2GdZM
"""

# @title
from google.colab import userdata
userdata.get('genai_course')
openai_api_key = userdata.get('genai_course')

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/GenAI/RAG/RAG with OpenAI

"""Perform OCR and transform to images


"""

!pip install pdf2image
!apt-get install -y poppler-utils

#Create a function to convert pdfs into images and stores the paths
def pdf_to_images(pdf_path, output_folder):
  image_paths = [] # Initialize image_paths here
  if not os.path.exists(output_folder):
    os.makedirs(output_folder)
    #Convert PDF into images
    images = convert_from_path(pdf_path)
    #Save images and paths
    for i, image in enumerate(images):
      image_path = os.path.join(output_folder, f"page_{i+1}.jpg")
      image.save(image_path, "JPEG")
      image_paths.append(image_path)
  else: # If the output folder already exists, get the list of image paths from it
    for filename in os.listdir(output_folder):
      if filename.endswith(".jpg"):
        image_paths.append(os.path.join(output_folder, filename))

  return image_paths

#Import the Libraries
from pdf2image import convert_from_path
import os

#At the end, we want to have
pdf_path = "Things mother used to make.pdf"
output_folder = "images"
image_paths = pdf_to_images(pdf_path, output_folder)

!pip install OpenAI

!pip install openai

#Import libraries
from openai import OpenAI
import base64 ### package needed to incode images

#Setup connection to openai API
client = OpenAI(api_key=openai_api_key)
model = "gpt-4o-mini"

#Read and incode one image
image_path = "images/page_23.jpg"
with open(image_path, "rb") as image_file:
  image_data = base64.b64encode(image_file.read()).decode('utf-8')
image_data

#Define the System Prompt
system_prompt = """Please analyze the content of this image and extract any related recipe information."""

#Call the OpenAI API using the chat completion method
response = client.chat.completions.create(
    model=model,
    messages=[
        #Provide the system prompt
          {"role": "system", "content": system_prompt},
        #The user messsage contains the text and image url/path
        {"role": "user", "content": [
            "This is the image from the recipe page.",
            {"type": "image_url",
             "image_url": {"url": f"data:image/jpeg;base64,{image_data}",
             "detail": "low"}}
        ]}
    ]
)

#Display the content
gpt_response=response.choices[0].message.content

from IPython.display import display, Markdown
display(Markdown(gpt_response))

#Define a function to the get the GPT Response and display in Markdown
def get_gpt_response():
  gpt_response = response.choices[0].message.content
  return display(Markdown(gpt_response))
get_gpt_response()

#Define improved system prompt
system_prompt = """
Please analyze the content of this image and extract any recipe related information into structured components.
Specifically, extract the recipe title, ingredients, cuisine type, dish type, any relevant tags and step-by-step directions.
The oputput should be formatted in a way suited for embedding in Retreival Augmented Generation system.
"""

#Call the API to extract the information
response = client.chat.completions.create(
    model=model,
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content":[
            "This is the image from the recipe page.",
              {"type": "image_url",
              "image_url": {"url": f"data:image/jpeg;base64,{image_data}",
                          "detail": "low"}}
        ]}
    ],
    temperature=0
)

extracted_recipes = []
#Reading and decoding images
for image_path in image_paths:
  print(f"Processing {image_path}")
  with open(image_path, "rb") as image_file:
    image_data = base64.b64encode(image_file.read()).decode('utf-8')

  #Call the OpenAI API
  response = client.chat.completions.create(
    model=model,
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content":[
            "This is the image from the recipe page.",
              {"type": "image_url",
              "image_url": {"url": f"data:image/jpeg;base64,{image_data}",
                          "detail": "low"}}
        ]}
    ],
    temperature=0
)
  # Extract The Content and store
  gpt_response = response.choices[0].message.content
  extracted_recipes.append({"image_path": image_path, "recipe_info": gpt_response})
  print(f"Extracted information from {image_path} :\n{gpt_response}\n")

"""Let's start by running the cell that generates the `image_paths`. Please run the following cell:"""

#Print the info from the page with the improved prompt
get_gpt_response()

extracted_recipes

#Filter out non-recipe content based on key-recipe related terms
filtered_recipes = []
for recipe in extracted_recipes:
  if any(keyword in recipe["recipe_info"].lower() for keyword in ["ingredients", "instructions", "recipe title"]):
    filtered_recipes.append(recipe)
  else:
    print(f"Skipping {recipe['image_path']} due to non-recipe content")

import json

#Define the output file path
output_file = "recipe_info.json"

# Write the filtered list to a JSON file
with open(output_file, "w") as json_file:
  json.dump(filtered_recipes, json_file, indent = 4)

"""Embeddings"""

#import libraries
import numpy as np

#Load The filtered recipes
with open("recipe_info.json", "r") as json_file:
  filtered_recipes = json.load(json_file)

"""Another option would be to organize per recipe but should be done in the preprocessing step"""

#Generate embeeding for each recipe
recipe_texts = [recipe["recipe_info"] for recipe in filtered_recipes]
embedding_response = client.embeddings.create(
    input=recipe_texts,
    model="text-embedding-3-large"
)

#Extract the embeddings
embeddings = [data.embedding for data in embedding_response.data]

#Convert The Embedding
embedding_matrix = np.array(embeddings)
embedding_matrix

#Verify The Embedding Matrix
print(f"Generated {len(embeddings)} embeddings of size {len(embeddings[0])} for {len(filtered_recipes)} recipes.")

"""Retrieval System"""

!pip install faiss-cpu

import faiss

#Print the embedding matrix shape
print(f"Embedding matrix shape: {embedding_matrix.shape}")

#initialize the FAISS index
index = faiss.IndexFlatL2(embedding_matrix.shape[1])
index.add(embedding_matrix)

#Save the index
faiss.write_index(index, "filtered_recipe_index.index")

#Save the Metadata |||| Allows you to assess outcomes to determine if results are reasonable
metadata = [{"recipe_info": recipe ["recipe_info"],
             "image_path": recipe["image_path"]} for recipe in filtered_recipes]
with open("filtered_recipe_metadata.json", "w") as json_file:
  json.dump(metadata, json_file, indent= 4)

#Define a function to query the embedddings
def query_embeddings(query, index, metadata, k=5):
  """
  Queries the FAISS index with a given query and returns the top k relevant recipes.

  Args:
    query (str): The query string.
    index (faiss.Index): The FAISS index.
    metadata (list): A list of dictionaries containing recipe metadata.
    k (int): The number of top results to retrieve.

  Returns:
    tuple: A tuple containing:
      - distances (numpy.ndarray): The distances of the retrieved items from the query.
      - results (list): A list of dictionaries, where each dictionary is the metadata
                       of a relevant recipe.
  """
  # Generate the embeddings for the query
  query_embedding = client.embeddings.create(
      input=[query],
      model="text-embedding-3-large"
  ).data[0].embedding
  # Convert the query embedding to a numpy array
  query_vector = np.array(query_embedding).reshape(1, -1)
  # Search the FAISS index
  distances, indices = index.search(query_vector, min(k, len(metadata)))

  # Retrieve the relevant metadata based on the indices
  results = [metadata[i] for i in indices[0]]

  return distances, results

query = "cakes"
distances, results = query_embeddings(query, index, metadata, k=3)

print(f"Query: {query}\n")
print("Top 3 relevant recipes:")
for i, (dist, recipe) in enumerate(zip(distances[0], results)):
    print(f"\nRank {i+1} (Distance: {dist:.4f}):")
    print(f"Image Path: {recipe['image_path']}")
    print(f"Recipe Info: {recipe['recipe_info'][:200]}...") # Displaying first 200 characters of recipe info

